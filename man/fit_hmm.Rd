% Generated by roxygen2 (4.1.0): do not edit by hand
% Please edit documentation in R/fit_hmm.R
\name{fit_hmm}
\alias{fit_hmm}
\title{Estimate Parameters of Hidden Markov Model}
\usage{
fit_hmm(model, em_step = TRUE, global_step = TRUE, local_step = TRUE,
  control_em = list(), control_global = list(), control_local = list(),
  lb, ub, ...)
}
\arguments{
\item{model}{Hidden Markov model of class \code{hmm}.}

\item{em_step}{Logical, use EM algorithm at the start of parameter estimation.
The default is \code{TRUE}. Note that EM algorithm is faster than direct numerical optimization,
but is even more prone to get stuck in a local optimum.}

\item{global_step}{Logical, use global optimization via
\code{\link{nloptr}} (possibly after the EM step). The default is \code{TRUE}.}

\item{local_step}{Logical, use local optimization via
\code{\link{nloptr}} (possibly after the EM and/or global steps). The default is \code{TRUE}.}

\item{control_em}{Optional list of control parameters for for EM algorithm.
Possible arguments are \describe{
\item{maxeval}{Maximum number of iterations, default is 100.}
\item{print_level}{Level of printing. Possible values are 0
(prints nothing), 1 (prints information at start and end of algorithm), and
2 (prints at every iteration).}
\item{reltol}{Relative tolerance for convergence defined as \eqn{(sumLogLikNew - sumLogLikOld)/(abs(sumLogLikOld)+0.1)}.
Default is 1e-8.} }}

\item{control_global}{Optional list of additional arguments for
  \code{\link{nloptr}} argument \code{opts}. The default values are
  \describe{
   \item{algorithm}{\code{"NLOPT_GD_MLSL_LDS"}}
   \item{local_opts}{\code{list(algorithm = "NLOPT_LD_LBFGS",  xtol_rel = 1e-4)}}
   \item{ranseed}{\code{123}}
   \item{maxeval}{\code{10000} (maximum number of iterations in global optimization algorithm)}
   \item{maxtime}{\code{60} (maximum run time in seconds)}
   \item{population}{\code{4*length(initialvalues)}} (number of starting points)
}}

\item{control_local}{Optional list of additional arguments for
\code{\link{nloptr}} argument \code{opts}. The default values are
\describe{
 \item{algorithm}{\code{"NLOPT_LD_LBFGS"}}
 \item{xtol_rel}{\code{1e-8}}
 \item{maxeval}{\code{10000} (maximum number of iterations)}
 \item{maxtime}{\code{60} (maximum run time in seconds)}
}}

\item{lb,ub}{Lower and upper bounds for parameters in Softmax parameterization.
Default interval is [pmin(-10,2*initialvalues), pmax(10,2*initialvalues)].
Used only in the global optimization step.}

\item{...}{Additional arguments to nloptr}
}
\value{
List with components \item{model}{Estimated model. }
  \item{logLik}{Log-likelihood of the estimated model. }
  \item{em_results}{Results after the EM step. }
  \item{global_results}{Results after the global step. }
  \item{local_results}{Results after the local step. }
}
\description{
Function \code{fit_hmm} estimates the initial state, transition and emission
probabilities of hidden Markov model. Initial values for estimation are taken from the
corresponding components of the model with preservation of original zero
probabilities. By default, the estimation start with EM algorithm and then switches
to direct numerical maximization.
}
\details{
The fitting function provides three estimation steps: 1) EM algorithm,
  2) global optimization, and 3) local optimization. The user can call for one method
  or any combination of these steps, but should note that they are preformed in the
  above-mentioned order. The results from a former step are used as starting values
  in a latter.

  By default the \code{fit_hmm} function starts with the EM algorithm,
  uses the multilevel single-linkage method (MLSL) with the LDS modification
  for global optimization (\code{NLOPT_GD_MLSL_LDS} as \code{algorithm} in
  \code{control_global}), and finishes with LBFGS as the local optimizer.
  The MLSL method draws random starting points and performs a local optimization
  from each. The LDS modification uses low-discrepancy sequences instead of
  pseudo-random numbers as starting points and should improve the convergence rate.
  By default, \code{fit_hmm} uses the BFGS algorithm as the local optimizer in the
  MLSL (\code{NLOPT_LD_LBFGS} as \code{local_opts} in \code{control_global}).
  In order to reduce the computation time spent on non-global optima, the
  convergence tolerance of the local optimizer is set relatively large. At step 3,
  a local optimization (BFGS by default) is run with a lower tolerance to find the
  optimum with high precision.

  Any method available in the \code{nloptr} function can be used for the global and
  local steps.
}
\examples{
data(biofam3c)

# Building sequence objects
child.seq <- seqdef(biofam3c$children)
marr.seq <- seqdef(biofam3c$married)
left.seq <- seqdef(biofam3c$left)

# Starting values for emission matrices
B_child <- matrix(NA, nrow = 3, ncol = 2)
B_child[1,] <- seqstatf(child.seq[, 1:5])[, 2] + 0.1
B_child[2,] <- seqstatf(child.seq[, 6:10])[, 2] + 0.1
B_child[3,] <- seqstatf(child.seq[, 11:15])[, 2] + 0.1
B_child <- B_child / rowSums(B_child)

B_marr <- matrix(NA, nrow = 3, ncol = 2)
B_marr[1,] <- seqstatf(marr.seq[, 1:5])[, 2] + 0.1
B_marr[2,] <- seqstatf(marr.seq[, 6:10])[, 2] + 0.1
B_marr[3,] <- seqstatf(marr.seq[, 11:15])[, 2] + 0.1
B_marr <- B_marr / rowSums(B_marr)

B_left <- matrix(NA, nrow = 3, ncol = 2)
B_left[1,] <- seqstatf(left.seq[, 1:5])[, 2] + 0.1
B_left[2,] <- seqstatf(left.seq[, 6:10])[, 2] + 0.1
B_left[3,] <- seqstatf(left.seq[, 11:15])[, 2] + 0.1
B_left <- B_left / rowSums(B_left)

# Starting values for transition matrix
A <- matrix(c(0.9, 0.07, 0.03,
                0,  0.9,  0.1,
                0,    0,    1), nrow = 3, ncol = 3, byrow = TRUE)

# Starting values for initial state probabilities
init <- c(0.9, 0.09, 0.01)

# Building hidden Markov model with initial parameter values
bHMM <- build_hmm(
  observations = list(child.seq, marr.seq, left.seq),
  transition_matrix = A,
  emission_matrix = list(B_child, B_marr, B_left),
  initial_probs = init
  )

# Fitting the model with different settings

# Only EM with default values
HMM1 <- fit_hmm(bHMM, em_step = TRUE, global_step = FALSE, local_step = FALSE)
HMM1$logLik #-5507.003

\dontrun{

# EM with LBFGS
HMM2 <- fit_hmm(bHMM, em_step = TRUE, global_step = FALSE, local_step = TRUE)
HMM2$logLik # -5507.003

# Only LBFGS
HMM3 <- fit_hmm(bHMM, em_step = FALSE, global_step = FALSE, local_step = TRUE)
HMM3$logLik #-5493.271

# Global optimization via MLSL_LDS with LBFGS as local optimizer and final polisher
HMM4 <- fit_hmm(bHMM, em_step = FALSE, global_step = TRUE, local_step = TRUE,
  control_global = list(maxeval = 3000, maxtime = 0))
HMM4$logLik #-5403.383

# As previously, but now we use ten iterations from EM algorithm for
# defining initial values and boundaries
# Note smaller maxeval for global optimization
HMM5 <- fit_hmm(bHMM, em_step = TRUE, global_step = TRUE, local_step = TRUE,
  control_em = list(maxeval = 5), control_global = list(maxeval = 750, maxtime = 0))
HMM5$logLik #-5403.383

}
}
\seealso{
\code{\link{build_hmm}} for building Hidden Markov models before
  fitting, \code{\link{trim_hmm}} for finding better models by changing small
  parameter values to zero, \code{\link{BIC.hmm}} for computing the
  value of the Bayesian information criterion of the model, and
  \code{\link{plot.hmm}} and \code{\link{ssplot}} for plotting
  hmm objects.
}

