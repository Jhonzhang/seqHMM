% Generated by roxygen2 (4.1.1): do not edit by hand
% Please edit documentation in R/fit_hmm.R
\name{fit_hmm}
\alias{fit_hmm}
\title{Estimate Parameters of Hidden Markov Model}
\usage{
fit_hmm(model, em_step = TRUE, global_step = FALSE, local_step = TRUE,
  control_em = list(), control_global = list(), control_local = list(),
  lb, ub, threads = 1, log_space = TRUE, ...)
}
\arguments{
\item{model}{Hidden Markov model.}

\item{em_step}{Logical, use EM algorithm at the start of parameter estimation.
The default is \code{TRUE}. Note that EM algorithm is faster than direct numerical optimization,
but is even more prone to get stuck in a local optimum.}

\item{global_step}{Logical, use global optimization via
\code{\link{nloptr}} (possibly after the EM step). The default is \code{FALSE}.}

\item{local_step}{Logical, use local optimization via
\code{\link{nloptr}} (possibly after the EM and/or global steps). The default is \code{TRUE}.}

\item{control_em}{Optional list of control parameters for for EM algorithm.
Possible arguments are \describe{
\item{maxeval}{Maximum number of iterations, default is 100.}
\item{print_level}{Level of printing. Possible values are 0
(prints nothing), 1 (prints information at start and end of algorithm), and
2 (prints at every iteration).}
\item{reltol}{Relative tolerance for convergence defined as \eqn{(logLik_new - logLik_old)/(abs(logLik_old)+0.1)}.
Default is 1e-8.}
\item{restarts}{Number of restarts of EM algorithm using random initial values. Default is 0. }
\item{restart_transition}{Logical, should the initial transition
probabilities be varied. Default is \code{TRUE}. }
\item{restart_emission}{Logical, should the initial emission
probabilities be varied. Default is \code{TRUE}. }
\item{sd_restart}{Standard deviation for \code{rnorm} used in restarting. Default is 0.25.} }}

\item{control_global}{Optional list of additional arguments for
  \code{\link{nloptr}} argument \code{opts}. The default values are
  \describe{
   \item{algorithm}{\code{"NLOPT_GD_MLSL_LDS"}}
   \item{local_opts}{\code{list(algorithm = "NLOPT_LD_LBFGS",  xtol_rel = 1e-4)}}
   \item{maxeval}{\code{10000} (maximum number of iterations in global optimization algorithm)}
}}

\item{control_local}{Optional list of additional arguments for
\code{\link{nloptr}} argument \code{opts}. The default values are
\describe{
 \item{algorithm}{\code{"NLOPT_LD_LBFGS"}}
 \item{xtol_rel}{\code{1e-8}}
 \item{maxeval}{\code{10000} (maximum number of iterations)}
}}

\item{lb,ub}{Lower and upper bounds for parameters in Softmax parameterization.
Default interval is [pmin(-10,2*initialvalues), pmax(10,2*initialvalues)].
Used only in the global optimization step.}

\item{threads}{Number of threads to use in parallel computing. Default is 1.}

\item{log_space}{Make computations using log-space instead of scaling for greater
numerical stability at cost of decreased computational performance Default is \code{TRUE}.}

\item{...}{Additional arguments to nloptr}
}
\value{
List with components \item{model}{Estimated model. }
  \item{logLik}{Log-likelihood of the estimated model. }
  \item{em_results}{Results after the EM step. }
  \item{global_results}{Results after the global step. }
  \item{local_results}{Results after the local step. }
}
\description{
Function \code{fit_hmm} estimates the initial state, transition and emission
probabilities of hidden Markov model. Initial values for estimation are taken from the
corresponding components of the model with preservation of original zero
probabilities. By default, the estimation start with EM algorithm and then switches
to direct numerical maximization.
}
\details{
The fitting function provides three estimation steps: 1) EM algorithm,
  2) global optimization, and 3) local optimization. The user can call for one method
  or any combination of these steps, but should note that they are preformed in the
  above-mentioned order. The results from a former step are used as starting values
  in a latter.

  By default the \code{fit_hmm} function starts with the EM algorithm,
  and finishes with LBFGS as the local optimizer.

  It is possible to rerun EM algorithm automatically using random starting
  values based on the first run of EM. Number of restarts is defined by
  argument \code{restarts} in \code{control_em}. As EM algorithm is relatively fast, this method
  might be preferred option compared to proper global optimization strategy of step 2.

  Default global optimization method (triggered via \code{global_step = TRUE} is
  the multilevel single-linkage method (MLSL) with the LDS modification (\code{NLOPT_GD_MLSL_LDS} as
  \code{algorithm} in \code{control_global}), with LBFGS as the local optimizer.
  The MLSL method draws random starting points and performs a local optimization
  from each. The LDS modification uses low-discrepancy sequences instead of
  pseudo-random numbers as starting points and should improve the convergence rate.
  In order to reduce the computation time spent on non-global optima, the
  convergence tolerance of the local optimizer is set relatively large. At step 3,
  a local optimization (LBFGS by default) is run with a lower tolerance to find the
  optimum with high precision.

  There are some theoretical guarantees that the MLSL method used as the default
  optimizer in step 2 shoud find all local optima in a finite number of local
  optimizations. Of course, it might not always succeed in a reasonable time.
  The EM algorithm can help in finding good boundaries for the search, especially
  with good starting values, but in some cases it can mislead. A good strategy is to
  try a couple of different fitting options with different combinations of the methods:
  e.g. all steps, only global and local steps, and a few evaluations of EM followed by
  global and local optimization.

  By default, the estimation time is limited to 60 seconds in global optimization step, so it is
  advisable to change the default settings for the proper global optimization.

  Any method available in the \code{nloptr} function can be used for the global and
  local steps.
}
\examples{
# Three-state three-channel hidden Markov model
# See ?hmm_biofam for five-state version

data(biofam3c)

# Building sequence objects
marr.seq <- seqdef(biofam3c$married, start = 15,
  alphabet = c("single", "married", "divorced"))
child.seq <- seqdef(biofam3c$children, start = 15,
  alphabet = c("childless", "children"))
left.seq <- seqdef(biofam3c$left, start = 15,
  alphabet = c("with parents", "left home"))

# Define colors
attr(marr.seq, "cpal") <- c("violetred2", "darkgoldenrod2", "darkmagenta")
attr(child.seq, "cpal") <- c("darkseagreen1", "coral3")
attr(left.seq, "cpal") <- c("lightblue", "red3")

# Starting values for emission matrices

emiss_marr <- matrix(NA, nrow = 3, ncol = 3)
emiss_marr[1,] <- seqstatf(marr.seq[, 1:5])[, 2] + 1
emiss_marr[2,] <- seqstatf(marr.seq[, 6:10])[, 2] + 1
emiss_marr[3,] <- seqstatf(marr.seq[, 11:16])[, 2] + 1
emiss_marr <- emiss_marr / rowSums(emiss_marr)

emiss_child <- matrix(NA, nrow = 3, ncol = 2)
emiss_child[1,] <- seqstatf(child.seq[, 1:5])[, 2] + 1
emiss_child[2,] <- seqstatf(child.seq[, 6:10])[, 2] + 1
emiss_child[3,] <- seqstatf(child.seq[, 11:16])[, 2] + 1
emiss_child <- emiss_child / rowSums(emiss_child)

emiss_left <- matrix(NA, nrow = 3, ncol = 2)
emiss_left[1,] <- seqstatf(left.seq[, 1:5])[, 2] + 1
emiss_left[2,] <- seqstatf(left.seq[, 6:10])[, 2] + 1
emiss_left[3,] <- seqstatf(left.seq[, 11:16])[, 2] + 1
emiss_left <- emiss_left / rowSums(emiss_left)

# Starting values for transition matrix
trans <- matrix(c(0.9, 0.07, 0.03,
                0,  0.9,  0.1,
                0,    0,    1), nrow = 3, ncol = 3, byrow = TRUE)

# Starting values for initial state probabilities
inits <- c(0.9, 0.09, 0.01)

# Building hidden Markov model with initial parameter values
init_hmm_bf <- build_hmm(
  observations = list(marr.seq, child.seq, left.seq),
  transition_probs = trans,
  emission_probs = list(emiss_marr, emiss_child, emiss_left),
  initial_probs = inits)

# Fitting the model with different optimization schemes

# Only EM with default values
hmm_1 <- fit_hmm(
  init_hmm_bf, local_step = FALSE)
hmm_1$logLik # -24179.1

\dontrun{

# EM with LBFGS
hmm_2 <- fit_hmm(init_hmm_bf)
hmm_2$logLik # -23017.97

# Only LBFGS
hmm_3 <- fit_hmm(init_hmm_bf, em_step = FALSE, local_step = TRUE)
hmm_3$logLik # -22267.75

# Global optimization via MLSL_LDS with LBFGS as local optimizer and final polisher
# This can be slow, use parallel computing by adjusting threads argument
# (threads = 1 for portability issues)
hmm_4 <- fit_hmm(
  init_hmm_bf, em_step = FALSE, global_step = TRUE,
  control_global = list(maxeval = 5000, maxtime = 0), threads = 8)
hmm_4$logLik # -21675.4

# EM with restarts, much faster than MLSL
set.seed(123)
hmm_5 <- fit_hmm(init_hmm_bf, control_em = list(restarts = 5, print_level = 1), threads = 1)
hmm_5$logLik # -21675.4

#' # Global optimization via STOGO with LBFGS as local optimizer and final polisher
# This can be slow, use parallel computing by adjusting threads argument
# (threads = 1 for portability issues)
set.seed(123)
hmm_6 <- fit_hmm(
   init_hmm_bf, em_step = FALSE, global_step = TRUE,
control_global = list(algorithm = "NLOPT_GD_STOGO", maxeval = 2500, maxtime = 0), threads = 8)
hmm_6$logLik # -21675.4
}
}
\seealso{
\code{\link{build_hmm}} for building Hidden Markov models before
  fitting, \code{\link{trim_hmm}} for finding better models by changing small
  parameter values to zero, and
  \code{\link{plot.hmm}} and \code{\link{ssplot}} for plotting
  hmm objects.
}

