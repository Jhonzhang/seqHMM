% Generated by roxygen2 (4.1.1): do not edit by hand
% Please edit documentation in R/fit_hmm.R
\name{fit_hmm}
\alias{fit_hmm}
\title{Estimate Parameters of Hidden Markov Model}
\usage{
fit_hmm(model, em_step = TRUE, global_step = TRUE, local_step = TRUE,
  control_em = list(), control_global = list(), control_local = list(),
  lb, ub, ...)
}
\arguments{
\item{model}{Hidden Markov model of class \code{hmm}.}

\item{em_step}{Logical, use EM algorithm at the start of parameter estimation.
The default is \code{TRUE}. Note that EM algorithm is faster than direct numerical optimization,
but is even more prone to get stuck in a local optimum.}

\item{global_step}{Logical, use global optimization via
\code{\link{nloptr}} (possibly after the EM step). The default is \code{TRUE}.}

\item{local_step}{Logical, use local optimization via
\code{\link{nloptr}} (possibly after the EM and/or global steps). The default is \code{TRUE}.}

\item{control_em}{Optional list of control parameters for for EM algorithm.
Possible arguments are \describe{
\item{maxeval}{Maximum number of iterations, default is 100.}
\item{print_level}{Level of printing. Possible values are 0
(prints nothing), 1 (prints information at start and end of algorithm), and
2 (prints at every iteration).}
\item{reltol}{Relative tolerance for convergence defined as \eqn{(sumLogLikNew - sumLogLikOld)/(abs(sumLogLikOld)+0.1)}.
Default is 1e-8.} }}

\item{control_global}{Optional list of additional arguments for
  \code{\link{nloptr}} argument \code{opts}. The default values are
  \describe{
   \item{algorithm}{\code{"NLOPT_GD_MLSL_LDS"}}
   \item{local_opts}{\code{list(algorithm = "NLOPT_LD_LBFGS",  xtol_rel = 1e-4)}}
   \item{ranseed}{\code{123}}
   \item{maxeval}{\code{10000} (maximum number of iterations in global optimization algorithm)}
   \item{maxtime}{\code{60} (maximum run time in seconds)}
   \item{population}{\code{4*length(initialvalues)}} (number of starting points)
}}

\item{control_local}{Optional list of additional arguments for
\code{\link{nloptr}} argument \code{opts}. The default values are
\describe{
 \item{algorithm}{\code{"NLOPT_LD_LBFGS"}}
 \item{xtol_rel}{\code{1e-8}}
 \item{maxeval}{\code{10000} (maximum number of iterations)}
 \item{maxtime}{\code{60} (maximum run time in seconds)}
}}

\item{lb,ub}{Lower and upper bounds for parameters in Softmax parameterization.
Default interval is [pmin(-10,2*initialvalues), pmax(10,2*initialvalues)].
Used only in the global optimization step.}

\item{...}{Additional arguments to nloptr}
}
\value{
List with components \item{model}{Estimated model. }
  \item{logLik}{Log-likelihood of the estimated model. }
  \item{em_results}{Results after the EM step. }
  \item{global_results}{Results after the global step. }
  \item{local_results}{Results after the local step. }
}
\description{
Function \code{fit_hmm} estimates the initial state, transition and emission
probabilities of hidden Markov model. Initial values for estimation are taken from the
corresponding components of the model with preservation of original zero
probabilities. By default, the estimation start with EM algorithm and then switches
to direct numerical maximization.
}
\details{
The fitting function provides three estimation steps: 1) EM algorithm,
  2) global optimization, and 3) local optimization. The user can call for one method
  or any combination of these steps, but should note that they are preformed in the
  above-mentioned order. The results from a former step are used as starting values
  in a latter.

  By default the \code{fit_hmm} function starts with the EM algorithm,
  uses the multilevel single-linkage method (MLSL) with the LDS modification
  for global optimization (\code{NLOPT_GD_MLSL_LDS} as \code{algorithm} in
  \code{control_global}), and finishes with LBFGS as the local optimizer.
  The MLSL method draws random starting points and performs a local optimization
  from each. The LDS modification uses low-discrepancy sequences instead of
  pseudo-random numbers as starting points and should improve the convergence rate.
  By default, \code{fit_hmm} uses the BFGS algorithm as the local optimizer in the
  MLSL (\code{NLOPT_LD_LBFGS} as \code{local_opts} in \code{control_global}).
  In order to reduce the computation time spent on non-global optima, the
  convergence tolerance of the local optimizer is set relatively large. At step 3,
  a local optimization (LBFGS by default) is run with a lower tolerance to find the
  optimum with high precision.

  There are some theoretical guarantees that the MLSL method used as the default
  optimizer in step 2 shoud find all local optima in a finite number of local
  optimizations. Of course, it might not always succeed in a reasonable time.
  The EM algorithm can help in finding good boundaries for the search, especially
  with good starting values, but in some cases it can mislead. A good strategy is to
  try a couple of different fitting options with different combinations of the methods:
  e.g. all steps, only global and local steps, and a few evaluations of EM followed by
  global and local optimization.

  By default, the estimation time is limited to 60 seconds in steps 2 and 3, so it is
  advisable to change the default settings for the final analysis.

  Any method available in the \code{nloptr} function can be used for the global and
  local steps.
}
\examples{
data(biofam3c)

# Building sequence objects
child.seq <- seqdef(biofam3c$children)
marr.seq <- seqdef(biofam3c$married)
left.seq <- seqdef(biofam3c$left)

# Starting values for emission matrices
emiss_child <- matrix(NA, nrow = 3, ncol = 2)
emiss_child[1,] <- seqstatf(child.seq[, 1:5])[, 2] + 0.1
emiss_child[2,] <- seqstatf(child.seq[, 6:10])[, 2] + 0.1
emiss_child[3,] <- seqstatf(child.seq[, 11:15])[, 2] + 0.1
emiss_child <- emiss_child / rowSums(emiss_child)

emiss_marr <- matrix(NA, nrow = 3, ncol = 3)
emiss_marr[1,] <- seqstatf(marr.seq[, 1:5])[, 2] + 0.1
emiss_marr[2,] <- seqstatf(marr.seq[, 6:10])[, 2] + 0.1
emiss_marr[3,] <- seqstatf(marr.seq[, 11:15])[, 2] + 0.1
emiss_marr <- emiss_marr / rowSums(emiss_marr)

emiss_left <- matrix(NA, nrow = 3, ncol = 2)
emiss_left[1,] <- seqstatf(left.seq[, 1:5])[, 2] + 0.1
emiss_left[2,] <- seqstatf(left.seq[, 6:10])[, 2] + 0.1
emiss_left[3,] <- seqstatf(left.seq[, 11:15])[, 2] + 0.1
emiss_left <- emiss_left / rowSums(emiss_left)

# Starting values for transition matrix
trans <- matrix(
  c(0.90, 0.07, 0.03,
       0, 0.90, 0.10,
       0,    0,    1),
  nrow = 3, ncol = 3, byrow = TRUE)

# Starting values for initial state probabilities
init <- c(0.9, 0.09, 0.01)

# Building hidden Markov model with initial parameter values
bhmm <- build_hmm(
  observations = list(child.seq, marr.seq, left.seq),
  transition_matrix = trans,
  emission_matrix = list(emiss_child, emiss_marr, emiss_left),
  initial_probs = init)

# Fitting the model with different settings

# Only EM with default values
hmm_1 <- fit_hmm(
  bhmm, em_step = TRUE, global_step = FALSE, local_step = FALSE)
hmm_1$logLik #-5507.003

\dontrun{

# EM with LBFGS
hmm_2 <- fit_hmm(
  bhmm, em_step = TRUE, global_step = FALSE, local_step = TRUE)
hmm_2$logLik # -5507.003

# Only LBFGS
hmm_3 <- fit_hmm(
  bhmm, em_step = FALSE, global_step = FALSE, local_step = TRUE)
hmm_3$logLik #-5493.271

# Global optimization via MLSL_LDS with LBFGS as local optimizer and final polisher
hmm_4 <- fit_hmm(
  bhmm, em_step = FALSE, global_step = TRUE, local_step = TRUE,
  control_global = list(maxeval = 3000, maxtime = 0))
hmm_4$logLik #-5403.383

# As previously, but now we use five iterations from EM algorithm for
# defining initial values and boundaries
# Note smaller maxeval for global optimization
hmm_5 <- fit_hmm(
  bhmm, em_step = TRUE, global_step = TRUE, local_step = TRUE,
  control_em = list(maxeval = 5), control_global = list(maxeval = 750, maxtime = 0))
hmm_5$logLik #-5403.383
}
}
\seealso{
\code{\link{build_hmm}} for building Hidden Markov models before
  fitting, \code{\link{trim_hmm}} for finding better models by changing small
  parameter values to zero, and
  \code{\link{plot.hmm}} and \code{\link{ssplot}} for plotting
  hmm objects.
}

